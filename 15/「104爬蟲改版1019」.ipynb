{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "avFMmJexupSJ"
      },
      "outputs": [],
      "source": [
        "# prompt: 用url 爬蟲\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "from urllib.parse import quote\n",
        "from tqdm import tqdm, trange\n",
        "import csv\n",
        "from datetime import date\n",
        "import os,os.path\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_url(url):\n",
        "  #根據url建立連線\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # 檢查 HTTP 狀態碼是否為 200\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    return soup\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"爬取 URL 失敗: {e}\")\n",
        "    return None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m9pQ7-_PuwSY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#安裝selenium、chromium-chromedriver驅動及更新\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.chrome.options import Options"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFWuTGuJuwB5",
        "outputId": "04dae53e-05ea-49fc-cc59-40daca2fe190"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sortedcontainers, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.25.0 sortedcontainers-2.4.0 trio-0.27.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,032 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,405 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,450 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,648 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,600 kB]\n",
            "Fetched 16.5 MB in 3s (5,229 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 9 newly installed, 0 to remove and 48 not upgraded.\n",
            "Need to get 29.0 MB of archives.\n",
            "After this operation, 120 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.65.3+22.04 [26.4 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 29.0 MB in 2s (19.3 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 123629 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 123837 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.65.3+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.65.3+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.65.3+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 124066 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def selenium_get_Code_104(url):\n",
        "    #chrome_options = Options() # 啟動無頭模式\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    # Change 'options' to 'chrome_options' and remove 'executable_path'\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    driver.get(url)\n",
        "    #要睡一下，有時候網路太爛他會抓到還沒跑完的網頁資料\n",
        "    time.sleep(1)\n",
        "    save = driver.page_source\n",
        "\n",
        "    driver.quit()#關閉瀏覽器\n",
        "    soup = BeautifulSoup(save, \"html.parser\")\n",
        "    page_select = soup.select_one('div.high-light.multiselect__content-wrapper > ul > li:nth-child(1) > span')\n",
        "    text = page_select.text\n",
        "    txtre =re.search(r'/\\s*(\\d+)',text)\n",
        "    #用re取\"第1/142頁\"中的142\n",
        "    page = txtre.group(1)\n",
        "    return page"
      ],
      "metadata": {
        "id": "vJMl5alMvBtN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datetime import date\n",
        "import csv\n",
        "from urllib.parse import quote\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "import random\n",
        "from google.colab import drive\n",
        "import requests\n",
        "import bs4\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util import Retry\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 設置日誌\n",
        "logging.basicConfig(filename='scraping.log', level=logging.INFO,\n",
        "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
        "\n",
        "today = date.today()\n",
        "\n",
        "# 關鍵字列表\n",
        "# key_texts = [\"軟體工程師\", \"Software Developer\", \"通訊軟體工程師\", \"韌體工程師\", \"Firmware Engineer\", \"軟體測試人員\", \"QA Engineer\", \"BIOS工程師\", \"BIOS Engineer\", \"CIM工程師\", \"MES工程師\", \"網站程式設計師\", \"Web Developer\"]  # 可以根據需要添加更多關鍵字\n",
        "key_texts = [\"DevOps工程師\", \"區塊鏈工程師\", \"Blockchain Engineer\"]  # 可以根據需要添加更多關鍵字\n",
        "\n",
        "def selenium_get_Code_104(url):\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    driver.get(url)\n",
        "    time.sleep(1)\n",
        "    save = driver.page_source\n",
        "\n",
        "    driver.quit()\n",
        "    soup = BeautifulSoup(save, \"html.parser\")\n",
        "    page_select = soup.select_one('div.high-light.multiselect__content-wrapper > ul > li:nth-child(1) > span')\n",
        "    text = page_select.text\n",
        "    txtre = re.search(r'/\\s*(\\d+)', text)\n",
        "    page = txtre.group(1)\n",
        "    return page\n",
        "\n",
        "def scrape_jobs(key_txt):\n",
        "    # URL編碼關鍵字\n",
        "    key = quote(key_txt)\n",
        "    # 設置CSV文件路徑\n",
        "    path_csv = f\"/content/drive/My Drive/colab data/{today}_{key_txt}_104人力銀行.csv\"\n",
        "    if not os.path.isdir('jobs_csv'):\n",
        "        os.mkdir('jobs_csv')\n",
        "        logging.info('建立jobs_csv資料夾完成')\n",
        "\n",
        "    # 創建CSV文件並寫入標題\n",
        "    with open(path_csv, mode='w', newline='', encoding='utf-8') as employee_file:\n",
        "        employee_writer = csv.writer(\n",
        "            employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "        employee_writer.writerow(['日期', '工作名稱', '公司名稱', '職務類別', '薪資', '工作內容', '地區',\n",
        "                                 '經歷', '學歷', '兩周內應徵人數', '文章編號', '工作網址', '產業別', '擅長工具', '工作技能', '科系要求', '其他條件'])\n",
        "\n",
        "    max_retries = 5\n",
        "    try:\n",
        "        url = f\"https://www.104.com.tw/jobs/search/?ro=0&kwop=7&keyword={key}&order=15&asc=0&page=1&mode=s&jobsource=2018indexpoc\"\n",
        "\n",
        "        # 使用 selenium_get_Code_104 函數獲取總頁數\n",
        "        total_pages = int(selenium_get_Code_104(url))\n",
        "        get_sum_page = min(total_pages, 150)  # 限制最多爬取150頁\n",
        "\n",
        "        logging.info(f'開始爬取關鍵字: {key_txt}, 共有：{get_sum_page} 頁')\n",
        "\n",
        "        for page in tqdm(range(1, get_sum_page + 1), desc=f\"爬取 {key_txt}\"):\n",
        "            url = f\"https://www.104.com.tw/jobs/search/?ro=0&kwop=7&keyword={key}&order=15&asc=0&page={page}&mode=s&jobsource=2018indexpoc\"\n",
        "\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    soup = read_url(url)\n",
        "                    if soup is None:\n",
        "                        raise Exception(\"Failed to retrieve page content\")\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    if attempt == max_retries - 1:\n",
        "                        logging.error(f\"爬取頁面失敗: {url}, 錯誤: {e}\")\n",
        "                        continue\n",
        "                    sleep_time = exponential_backoff(attempt)\n",
        "                    time.sleep(sleep_time)\n",
        "\n",
        "            if soup is None:\n",
        "                continue\n",
        "\n",
        "            for title_1 in soup.select('.job-summary'):\n",
        "                try:\n",
        "                    # 提取各種職位信息\n",
        "                    date = '廣告' if title_1.select(\n",
        "                        '.col-auto.date')[0].select('i') else title_1.select('.col-auto.date')[0].get_text()\n",
        "                    area = title_1.select(\n",
        "                        '.info-tags.gray-deep-dark')[0].find('a').get_text()\n",
        "                    experience = title_1.select(\n",
        "                        '.info-tags.gray-deep-dark')[0].find_all('a')[1].get_text()\n",
        "                    education = title_1.select('.info-tags.gray-deep-dark')[0].find_all('a')[2].get_text(\n",
        "                    ) if len(title_1.select('.info-tags.gray-deep-dark')[0].find_all('a')) > 2 else \"\"\n",
        "                    title_url = title_1.select(\n",
        "                        '.info-job__text.jb-link.jb-link-blue.jb-link-blue--visited.h2')[0]['href']\n",
        "                    title_str = title_url.split('?')[0].split('/')[-1]\n",
        "                    title = title_1.select(\n",
        "                        '.info-job__text.jb-link.jb-link-blue.jb-link-blue--visited.h2')[0]['title']\n",
        "                    company_name = title_1.select(\n",
        "                        '.info-company__text.jb-link.jb-link-blue.jb-link-blue--visited.h4')[0].get_text()\n",
        "\n",
        "                    salary_element = title_1.select(\n",
        "                        '.info-tags.gray-deep-dark')[0].find_all('a')\n",
        "                    salary = salary_element[3].get_text() if len(\n",
        "                        salary_element) > 3 else \"待遇面議\"\n",
        "                    if salary != \"待遇面議\":\n",
        "                        salary = re.search(\n",
        "                            r'\\d+.\\d+', salary).group() if re.search(r'\\d+.\\d+', salary) else \"0\"\n",
        "\n",
        "                    people = title_1.select('.action-apply__range.d-flex.text-center.align-items-center')[\n",
        "                        0]['title'] if title_1.select('.action-apply__range.d-flex.text-center.align-items-center') else \"\"\n",
        "                    industry = title_1.select(\n",
        "                        '.info-company-addon-type.text-gray-darker.font-weight-bold')[0].get_text()\n",
        "                    industry = industry if \"業\" in industry else \"無\"\n",
        "\n",
        "                    # 爬取工作網址內的內容\n",
        "                    detail_soup = read_url(title_url)\n",
        "\n",
        "                    introduction, tools, skills, job_categories, major_requirement, other_requirements = extract_job_details(\n",
        "                        detail_soup)\n",
        "\n",
        "                    # 準備寫入CSV的行數據\n",
        "                    row = [date, title, company_name, job_categories, salary, introduction, area, experience, education,\n",
        "                           people, title_str, title_url, industry, tools, skills, major_requirement, other_requirements]\n",
        "\n",
        "                    # 將數據寫入CSV文件\n",
        "                    with open(path_csv, mode='a', newline='', encoding='utf-8') as employee_file:\n",
        "                        employee_writer = csv.writer(\n",
        "                            employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "                        employee_writer.writerow(row)\n",
        "\n",
        "                    time.sleep(random.uniform(1, 3))  # 隨機延遲1-3秒，避免過快請求\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"處理職位資訊時發生錯誤: {e}\")\n",
        "                    continue\n",
        "\n",
        "        logging.info(f\"完成爬取關鍵字: {key_txt}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"爬蟲過程中發生錯誤: {e}\")\n",
        "\n",
        "def extract_job_details(detail_soup):\n",
        "    introduction = \"\"\n",
        "    tools = []\n",
        "    skills = []\n",
        "    job_categories = []\n",
        "    major_requirement = \"\"\n",
        "    other_requirements = \"\"\n",
        "\n",
        "    if detail_soup:\n",
        "        job_description = detail_soup.select_one('p.job-description__content')\n",
        "        if job_description:\n",
        "            introduction = job_description.get_text(strip=True)\n",
        "        else:\n",
        "            introduction = \"未提供工作內容\"\n",
        "\n",
        "        list_rows = detail_soup.select('div.list-row.row.mb-2')\n",
        "        tools = []\n",
        "        skills = []\n",
        "        job_categories = []\n",
        "        major_requirement = \"未指定\"\n",
        "        other_requirements = \"不拘\"\n",
        "\n",
        "        for row in list_rows:\n",
        "            header = row.select_one('div.col.p-0.mr-4.list-row__head h3')\n",
        "            if header:\n",
        "                header_text = header.get_text().strip()\n",
        "                data_block = row.select_one('div.col.p-0.list-row__data')\n",
        "\n",
        "                if data_block:\n",
        "                    if \"擅長工具\" in header_text:\n",
        "                        tools = [u.get_text().strip()\n",
        "                                 for u in data_block.select('a.tools u')]\n",
        "                    elif \"工作技能\" in header_text:\n",
        "                        skills = [u.get_text().strip()\n",
        "                                  for u in data_block.select('a.skills u')]\n",
        "                    elif \"職務類別\" in header_text:\n",
        "                        job_categories = [u.get_text().strip()\n",
        "                                          for u in data_block.select('u')]\n",
        "                    elif \"科系要求\" in header_text:\n",
        "                        major_element = data_block.select_one('div.t3.mb-0')\n",
        "                        major_requirement = major_element.get_text(\n",
        "                            strip=True) if major_element else \"未指定\"\n",
        "                    elif \"其他條件\" in header_text:\n",
        "                        other_element = data_block.select_one(\n",
        "                            'div.col.p-0.job-requirement-table__data p.t3.m-0') or data_block.select_one('div.t3.mb-0')\n",
        "                        other_requirements = other_element.get_text(\n",
        "                            strip=True) if other_element else \"不拘\"\n",
        "\n",
        "        # 將列表轉換為字符串，並處理空值\n",
        "        tools = ', '.join(tools) if tools else \"不拘\"\n",
        "        skills = ', '.join(skills) if skills else \"不拘\"\n",
        "        job_categories = ', '.join(job_categories) if job_categories else \"未指定\"\n",
        "        other_requirements = \"不拘\" if other_requirements.strip() in [\n",
        "            \"\", \"未填寫\"] else other_requirements\n",
        "\n",
        "    return introduction, tools, skills, job_categories, major_requirement, other_requirements\n",
        "\n",
        "\n",
        "# 主程序\n",
        "for key_txt in key_texts:\n",
        "    scrape_jobs(key_txt)\n",
        "\n",
        "logging.info(\"所有關鍵字爬蟲完成\")\n",
        "\n",
        "\n",
        "def exponential_backoff(attempt):\n",
        "    return min(30, (2 ** attempt) + random.uniform(0, 1))\n",
        "\n",
        "\n",
        "def read_url(url):\n",
        "    session = create_retry_session()\n",
        "    headers = get_random_headers()\n",
        "    try:\n",
        "        response = session.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        return bs4.BeautifulSoup(response.text, 'html.parser')\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        if e.response is not None and e.response.status_code == 429:\n",
        "            logging.warning(\"遇到 429 錯誤，嘗試使用新的 header\")\n",
        "            headers = get_random_headers()  # 獲取新的隨機 header\n",
        "            try:\n",
        "                response = session.get(url, headers=headers)\n",
        "                response.raise_for_status()\n",
        "                return bs4.BeautifulSoup(response.text, 'html.parser')\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                logging.error(f\"使用新 header 後仍然爬取 URL 失敗: {e}\")\n",
        "                return None\n",
        "        else:\n",
        "            logging.error(f\"爬取 URL 失敗: {e}\")\n",
        "            return None\n",
        "\n",
        "def create_retry_session(retries=3, backoff_factor=0.3):\n",
        "    retry_strategy = Retry(\n",
        "        total=retries,\n",
        "        backoff_factor=backoff_factor,\n",
        "        status_forcelist=[429, 500, 502, 503, 504],\n",
        "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\", \"POST\", \"PUT\", \"DELETE\"]\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "    session = requests.Session()\n",
        "    session.mount(\"https://\", adapter)\n",
        "    session.mount(\"http://\", adapter)\n",
        "    return session\n",
        "\n",
        "def get_random_headers():\n",
        "    user_agents = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
        "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36',\n",
        "        'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'\n",
        "    ]\n",
        "    return {\n",
        "        'User-Agent': random.choice(user_agents),\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.5',\n",
        "        'Accept-Encoding': 'gzip, deflate, br',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Upgrade-Insecure-Requests': '1',\n",
        "        'Cache-Control': 'max-age=0'\n",
        "    }\n"
      ],
      "metadata": {
        "id": "OVRntPA2vEAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86c8012a-8f06-4f4b-dc5c-0e8088e62a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "爬取 DevOps工程師: 100%|██████████| 31/31 [34:30<00:00, 66.79s/it]\n",
            "爬取 區塊鏈工程師: 100%|██████████| 4/4 [03:47<00:00, 56.80s/it]\n",
            "爬取 Blockchain Engineer:   7%|▋         | 11/150 [12:52<2:34:21, 66.63s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url='https://www.104.com.tw/jobs/search/?jobsource=index_s&keyword=data&mode=s&page=3&order=15'\n",
        "soup=read_url(url)\n",
        "for title_1 in soup.select('.job-summary'):\n",
        "  a= title_1.select('.info-company-addon-type.text-gray-darker.font-weight-bold')[0].get_text()\n",
        "\n",
        "\n",
        "\n",
        "  print(a)"
      ],
      "metadata": {
        "id": "-tjc_8x9vGkj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}